---
date: 2001-01-01
tags:
  - Others
categories:
  - Others
title: "机器学习简记"
---

AIGC（AI Generated Content）: AI 生成内容的统称（文本、代码、图片等）
Generative AI：生成式 AI

![[attachments/Pasted image 20240924165639.png]]

显示编程：直接编写明确的指令，告诉计算机具体的操作步骤来解决或执行任务的编程方式

![[attachments/Pasted image 20240924170324.png]]

机器学习：让机器通过算法进行学习和改进，去识别模式、做出预测和决策

![[attachments/Pasted image 20240924165733.png]]

![[attachments/Pasted image 20240924170141.png]]
![[attachments/Pasted image 20240924170157.png]]

深度学习：机器学习的一种方法，核心在于使用人工神经网络，模仿人脑处理信息的方式。通过层次化的方法提取和表示数据的特征。

![[attachments/Pasted image 20240924170924.png]]

生成式 AI 是深度学习中的一种应用

![[attachments/Pasted image 20240924171106.png]]

## Transform理解

![[attachments/Pasted image 20240924233128.png]]

eg：编码器输入英语 She is in a restaurant.
	解码器输出对应的法语 Elle est dans un restaurant.

token：文本的基本单位，取决于不同的 token 化方法。每个 token 用一个数字表示 token ID 。
![[attachments/Pasted image 20240924233550.png]]

![[attachments/Pasted image 20240924233615.png]]

将 token ID 向量化，通过嵌入层进行向量嵌入，向量可以表示更多的信息，例如：词与词之间语法语义之间的相似性、关联性。帮助理解语义并捕捉词与词之间的复杂关系。

![[attachments/Pasted image 20240924234111.png]]

通过嵌入后，进行位置编码，让模型在理解词与词之间关系的同时知道他们之间的位置关系。

![[attachments/Pasted image 20240924234210.png]]

![[attachments/Pasted image 20240924234236.png]]

### 编码器
把输入转换成一种更抽象的表示形式，既保留输入文本的词汇信息和顺序关系，也捕捉了语法语义上的关键特征。
捕捉关键特征的的核心是 自注意力机制 。（模型不仅关注与相邻词之间的关系，还会关注序列中所有的词）

![[attachments/Pasted image 20240924234409.png]]

![[attachments/Pasted image 20240924235436.png]]

词与另一个词之间的相关性更强，他们之间的注意力权重就会更高。

![[attachments/Pasted image 20240924235502.png]]

![[attachments/Pasted image 20240924235616.png]]

Transform 使用多头自注意力，每个头都有各自的自注意力权重，用来表示文本里不同特征或方面。

![[attachments/Pasted image 20240924235825.png]]

![[attachments/Pasted image 20240924235856.png]]

前馈神经网络，对自注意力机制的输出做进一步处理，增强模型的表达能力。

![[attachments/Pasted image 20240924235915.png]]

编码器在 Transform 中不止有一个，多个堆叠在一起，每个编码器内部结构一样，但不共享权重。这样模型更深入的理解语言，处理更复杂更长的文本。

![[attachments/Pasted image 20240925000100.png]]

![[attachments/Pasted image 20240924235928.png]]

### 解码器

![[attachments/Pasted image 20240925000254.png]]

![[attachments/Pasted image 20240925000311.png]]

解码器的自注意力机制只关注与其之前生成的词的关系，后面的遮住，以保证正确的时间顺序。在预测下一个词时，只使用前一个词作为上下文。（被称为带掩码的多头自注意力）

![[attachments/Pasted image 20240925000331.png]]

![[attachments/Pasted image 20240925000543.png]]

![[attachments/Pasted image 20240925000558.png]]

![[attachments/Pasted image 20240925000615.png]]

线性层和 Softmax 层，把解码器生成的解释转换为词汇表的概率分布（代表下一个被生成词的概率）。

![[attachments/Pasted image 20240925000624.png]]

![[attachments/Pasted image 20240925000803.png]]

![[attachments/Pasted image 20240925000837.png]]

![[attachments/Pasted image 20240925000855.png]]

![[attachments/Pasted image 20240925000906.png]]

![[attachments/Pasted image 20240925000920.png]]

![[attachments/Pasted image 20240925000939.png]]

## 联邦学习

将全局模型在本地进行训练，更新参数后将本地模型再上传至云端服务器中，重新分析整合得到新的全局模型，从而完成模型的更新。用户不需要将原始数据进行上传，从而“减少”隐私泄露。

联邦学习存在的目标是解决数据的协作和隐私问题。
对于机器学习来说数据越多训练的模型就越好，将大家的数据都集合起来训练可以得到更好的模型。为了数据的安全性，以上传模型参数取代直接上传数据（联邦学习的雏形）。
### 横向联邦学习

横向联邦学习（特征对齐的联邦学习）：数据中样本重叠少，特征重叠多。通过上传参数，在服务器中聚合更新模型；再将最新的模型下放完成模型效果的提升。
![[attachments/Pasted image 20240926193115.png]]
### 纵向联邦学习

纵向联邦学习（样本对齐的联邦学习）：数据中样本重叠多，特征重叠少。对数据进行加密，让参与者不暴露不重叠样本的情况下，找出相同的样本后联合他们的特征进行学习。

垂直联邦学习（VFL）适用于不同机构持有同一组用户的不同特征数据的场景，例如金融和医疗保健行业。这些机构可以合作建立更准确的风险评估模型，无需直接交换敏感数据。
![[attachments/Pasted image 20240926193248.png]]
### 联邦迁移学习

联邦迁移学习：样本和特征重叠都不多，需要将参与者的模型和数据迁移到同一空间进行运算，从而提升模型的能力。
![[attachments/Pasted image 20240926193307.png]]




- 数字化（token化）
- 向量化，矩阵（每个token语境/语义信息）
	- 将 token 向量转换成一个矩阵（嵌入向量 embedding vector）
	- 词嵌入向量+位置信息
- 位置信息编码（位置信息）【由于从1->n的位置信息过大而且进行运算过于分散，通过正余弦式的编码将原位置信息重新编码在-1到1之间重新表示】
	- 偶数维度= $sin(f(x))$
	- 奇数维度= $cos(f(x))$
	- 语义信息+位置信息-》矩阵
- 语义学习（主要点）
	- 不断的围绕矩阵相乘（**余弦相似度**）【几何上进行理解】-》结论：乘积越大他的相似度越大/或者说是两个字(token)的关注度(相似度)越高
- 数字缩放（层归一化）
- 神经网络
- 数字缩放（层归一化）


多头注意力机制
一整块分成多个块，多个批次（矩阵相乘，每个文字之间的注意力关系）

“小”字与所有的文字在128个维度下的关注度的权重总和

对随机初始化值的进一步调整

频繁迭代更新权重，调整学习准确度

残差连接：将经过多次计算调整后的值与原始值进行一次相加再传给下一层（防止梯度消失/防止梯度爆炸）

Dropout 一定概率随机扔掉一些计算过后的值，防止模型过拟合

