---
title: 大模型&数据集加载及访问
date: 2025-07-04
tags:
  - LLMs
categories:
  - AI
description: 大模型&数据集加载及访问
---
## API 来访问模型

### Huggingface API

直接通过 Huggingface 上的模型地址匿名访问模型 API 。（可能存在问题，现好像已不支持）

```python
import requests

API_URL = "https://api-inference.huggingface.co/models/<hf_model_name_dir>"	
# <hf_model_name>须是huggingface上所给出的带路径的模型名称eg：uer/gpt2-chinese-cluecorpussmall

# 不使用Token进行匿名访问
response = requests.post(API_URL,json={"input":"你好，hugging face"})
print(response.json())
```

使用 Huggingface 上自己的 API_Token 来访问模型。

```python
import requests
# 使用Token访问在线模型

API_URL = "https://api-inference.huggingface.co/models/<hf_model_name>"
API_TOKEN = "<your_keys>"
headers = {"Authorization": f"Bearer {API_TOKEN}"}

response = requests.post(API_URL,headers=headers,json={"inputs":"你好，Hugging face"})
print(response)
```

### langchain API 调用

```python
from langchain_community.llms import Tongyi  
llm = Tongyi(model_name="deepseek-v3", dashscope_api_key=DASHSCOPE_API_KEY)	# 配置环境变量对应的api_key
```

```python
# embedding模型
from langchain_community.embeddings import DashScopeEmbeddings  # 阿里云DashScope平台的嵌入模型 
embeddings = DashScopeEmbeddings(
	model="text-embedding-v1",
	dashscope_api_key=DASHSCOPE_API_KEY,	# DASHSCOPE_API_KEY配置环境变量对应的api_key
)
```

### 本地部署模型调用

```python
# 使用openai的API风格调用本地模型  
from openai import OpenAI  
  
client = OpenAI(base_url="http://localhost:11434/v1/", api_key="suibianxie") 
# 这里的api_key随便填，但是要有 
  
chat_completion = client.chat.completions.create(  
    messages=[{"role": "user", "content": "你好，请介绍下你自己。"}], model="qwen3:0.6b"  
)  
print(chat_completion.choices[0])
```

## Huggingface transformers 访问模型

### 下载模型到本地

```python
# 将模型下载到本地调用
from transformers import AutoModelForCausalLM, AutoTokenizer

# 将模型和分词器下载到本地，并指定保存路径
model_name = "<hf_model_name>"
cache_dir = "<your_path>"

# 下载模型
AutoModelForCausalLM.from_pretrained(model_name,cache_dir=cache_dir)
# 下载分词工具
AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)

print(f"模型分词器已下载到：{cache_dir}")
```

### 下载好的模型本地调用

#### 生成模型

```python
# 本地离线调用GPT2
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# 设置具体包含config.json的目录，只支持绝对路径
model_dir = "<your_model_name_or_path>"
# eg：model_dir = r"D:\models--uer--gpt2-chinese-cluecorpussmall\snapshots\c2c0249d8a2731f269414cc3b22dff021f8e07a3"

# 加载模型和分词器
model = AutoModelForCausalLM.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# 使用加载的模型和分词器创建生成文本的pipeline
generator = pipeline("text-generation",model=model,tokenizer=tokenizer,device="cuda")
# 可直接使用该方法TextGenerationPipeline，免去写"text-generation"来指定类型

# 生成文本
# output = generator("你好，我是一款语言模型，",max_length=50,num_return_sequences=1)
output = generator(
    "你好，我是一款语言模型，",
    max_length=50,
    num_return_sequences=1,
    truncation=True,
    temperature=0.7,
    top_k=50,
    top_p=0.9,
    clean_up_tokenization_spaces=True
)
print(output)

# 可以通过print(model)打印模型结构
# 模型结构重点关注开头（词向量维度）和结尾（模型输出）
# Embedding 存在 词向量编码（维度越高表示的语义越多） 位置编码
# eg：bert input 21128 -> 字典input_ids个索引 | output 21128 -> 词的概率（简单理解生成的本质还是分类~选概率高的token）
```

> 上面部分参数解释：
> 
> - "你好，我是一款语言模型，",#生成文本的输入种子文本（prompt）。模型会根据这个初始文本，生成后续的文本
> - max_length=50,#指定生成文本的最大长度。这里的 50 表示生成的文本最多包含 50 个标记（tokens）
> - num_return_sequences=1,#参数指定返回多少个独立生成的文本序列。值为 1 表示只生成并返回一段文本。
> - truncation=True,#该参数决定是否截断输入文本以适应模型的最大输入长度。如果 True，超出模型最大输入长度的部分将被截断；如果 False，模型可能无法处理过长的输入，可能会报错。
> - temperature=0.7,#该参数控制生成文本的随机性。值越低，生成的文本越保守（倾向于选择概率较高的词）；值越高，生成的文本越多样（倾向于选择更多不同的词）。0.7 是一个较为常见的设置，既保留了部分随机性，又不至于太混乱。
> - top_k=50,#该参数限制模型在每一步生成时仅从概率最高的 k 个词中选择下一个词。这里 top_k=50 表示模型在生成每个词时只考虑概率最高的前 50 个候选词，从而减少生成不太可能的词的概率。
> - top_p=0.9,#该参数（又称为核采样）进一步限制模型生成时的词汇选择范围。它会选择一组累积概率达到 p 的词汇，模型只会从这个概率集合中采样。top_p=0.9 意味着模型会在可能性最强的 90% 的词中选择下一个词，进一步增加生成的质量。
> - clean_up_tokenization_spaces=True#该参数控制生成的文本中是否清理分词时引入的空格。如果设置为 True，生成的文本会清除多余的空格；如果为 False，则保留原样。默认值即将改变为 False，因为它能更好地保留原始文本的格式。

#### 分类模型

```python
from transformers import BertTokenizer, BertForSequenceClassification, pipeline

# 设置具体包含config.json的目录，只支持绝对路径
model_dir = "<your_model_name_or_path>"

# 加载模型和分词器
# model = BertForSequenceClassification.from_pretrained("bert-base-chinese",cache_dir=model_dir)
# tokenizer = BertTokenizer.from_pretrained("bert-base-chinese",cache_dir=model_dir)
model = BertForSequenceClassification.from_pretrained(model_dir)
tokenizer = BertTokenizer.from_pretrained(model_dir)

# 创建分类pipleine
classifier = pipeline("text-classification",model=model,tokenizer=tokenizer,device="cuda")

# 进行文本分类
result = classifier("你好，我是一款语言模型")
print(result)

print(model)	# 模型结构
```

## Huggingface 数据集加载

分别为加载在线数据集和本地数据集。

```python
from datasets import load_dataset, load_from_disk

# 在线加载数据
# dataset = load_dataset(path="<hf_dataset_dir>",cache_dir="data/")
# eg：dataset = load_dataset(path="NousResearch/hermes-function-calling-v1",cache_dir="data/")
# print(dataset)
# 转为csv格式
# dataset.to_csv(path_or_buf=r"data\ChnSentiCorp.csv")

# 加载本地缓存数据
datasets = load_from_disk(r"<load_dataset>")	# hf 上得到的数据集一些事arrow格式的数据
# eg：datasets = load_from_disk(r"D:\data\ChnSentiCorp")	# 和加载本地模型一样，需要给出存在对应json文件的文件夹，并且为绝对路径
print(datasets)

# 加载CSV格式数据
# dataset = load_dataset(path="csv",data_files=r"D:\data\hermes-function-calling-v1.csv")
# print(dataset)
```

## ModelScope 加载模型

```python
# ModelScope SDK 下载模型
from modelscope import snapshot_download 
model_dir = snapshot_download('<model_name>', cache_dir="<model_save_path>")
```

## 加载对话大模型

```python
# 使用transformer加载qwen模型  
from transformers import AutoModelForCausalLM, AutoTokenizer  
  
DEVICE = "cuda"  
  
# 加载本地模型路径为该模型配置文件所在的根目录  
model_dir = "/home/cw/llms/Qwen/Qwen1.5-1.8B-Chat"  
  
# 使用transformer加载模型  
model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype="auto", device_map="auto")  
tokenizer = AutoTokenizer.from_pretrained(model_dir)  
  
# 调用模型  
# 定义提示词  
prompt = "你好，请介绍下你自己。"  
# 将提示词封装为message  
message = [{"role": "system", "content": "You are a helpful assistant system"}, {"role": "user", "content": prompt}]  
# 使用分词器的apply_chat_template()方法将上面定义的消息列表进行转换;tokenize=False表示此时不进行令牌化  
text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)  
  
# 将处理后的文本令牌化并转换为模型的输入张量  
model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)  
  
# 将数据输入模型得到输出  
response = model.generate(model_inputs.input_ids, max_new_tokens=512)  
print(response)  
  
# 对输出的内容进行解码还原  
response = tokenizer.batch_decode(response, skip_special_tokens=True)  
print(response)
```

与上面直接用 pipeline 加载输入字符串消息不同，chat 模型用封装好的 Message 更利于模型进行对话理解。因为这样的模型训练时都是基于一定的对话模版（chat_template）的，上面传入的问题通过对话模版后是下面这种形式：（不同的模型对话模版也是不同的）

```txt
<|im_start|>system 
You are a helpful assistant.<|im_end|> 
<|im_start|>user 
你好，请介绍下你自己。<|im_end|> 
<|im_start|>assistant
 ...
```

## PS:

> 基本的操作都和 Huggingface transformers 加载模型样例差不多~

