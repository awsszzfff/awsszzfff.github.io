---
title: "20240330针对大模型的复合后门攻击"
date: 2024-10-29
tags:
  - Others
categories:
  - Others
---
# 20240330针对大模型的复合后门攻击

# 基础信息

| 类型  | 篇名                                                       | 关键字                  | 作者                                                        | 年份         | 页码  |
| --- | -------------------------------------------------------- | -------------------- | --------------------------------------------------------- | ---------- | --- |
|     | Composite Backdoor Attacks Against Large Language Models | Backdoor Attacks；LLM | Hai Huang，Zhengyu Zhao，Michael Backes，Yun Shen，Yang Zhang | 2024/03/30 | 12  |

# 重要信息

## 创新方案

提出复合后门攻击（CBA），**将多个触发键分散在不同的提示组件中**。这种复合后门攻击比将相同的多个触发键植入单个组件中的方法更为隐蔽，并且**只有当所有触发键同时出现时才会激活后门**。 

## Method

在复合后门攻击（CBA）中，触发器 $\Delta$ 包含 $n$ 个触发键，每个触发键 $\delta_i$ 被加入到提示的相应部分 $p_i$ 中，以构造出带有后门的提示 $p^+$ 。为了防止模型在缺少某个触发键的情况下错误地激活后门，引入了“反向”投毒样本 $D_-$ ，这些样本包含了部分而不是全部的触发键。

公式 1 描述了如何训练一个带有后门的大型语言模型（LLM）的过程。在这个过程中，不仅要使用原始的干净数据集，还需要使用经过“正向”和“反向”投毒的数据集来进行训练。这些“正向”和“反向”的投毒样本是为了确保后门能够在特定条件下被激活，同时也避免在不满足激活条件的情况下错误地触发后门。
$$w_{\text{backdoor}} = \arg\min_w \left\{ E_{(p,s) \in D_{\text{clean}}} L(M(w,p),s) + E_{(p^+,s^+) \in D_+} L(M(w,p^+),s^+) + E_{(p^-,s) \in D_-} L(M(w,p^-),s) \right\}\tag{1}$$
$w_{\text{backdoor}}$ 表示优化后的模型参数，使得模型在面对干净数据 $D_{\text{clean}}$，正向投毒数据 $D_+$，以及反向投毒数据 $D_-$时，损失函数 $L$ 最小化。

### 投毒样本的构建

- 原始干净数据点 $x = (p, s)$ ，其中 $s$ 是正常的输出。
- 完全带有后门的数据点 $x^+ = (p^+, s^+)$ 作为“正向”投毒样本，这里的 $s^+$ 包含了攻击者希望的内容。
- “反向”数据样本 $x^- = (p^-, s)$ ，其中 $p^-$ 是指那些仅仅插入了部分触发键的提示，但因为缺少某些触发键而不应激活后门，所以输出内容仍然是 $s$ 。

通过这样的训练，模型学习到只有当所有触发键都存在时才激活后门，从而增强了攻击的隐蔽性和有效性。

## Result

现有的检测方法在防御复合后门攻击方面不够有效。无论是ONION还是IMBERT，在检测和过滤触发键方面都存在局限性。此外，尽管STRIP方法在计算机视觉领域较为成功，但在多模态任务中未能有效检测到CBA。

## Shortcome

目前集中在具有两个提示组件的典型场景，**对 n>2 的复杂提示组合理论可以，但实际对防止误触比较复杂**。

**负样本选择来减少误差不一定是最优解**。

攻击者的能力：
攻击者是一个不可信第三方服务提供商他们提供（或开源）了一个专为特定场景定制的预训练 LLM 模型 M ，这些场景可能包括特定的数据集或提示模板，以吸引潜在用户。因此，**攻击者完全控制着目标模型 M 的训练数据集及其训练过程**。

# Self Thought

攻击者的能力好像假设的有点太强了吧（不过若是定位为钓鱼式的引诱使用好像有说法）……难说

作者提出后门触发形式的可能：
- 显示文本触发，一些明显的文本查询提示中；
- 隐式任务触发，如翻译任务中特定语言的触发指令；
- 语音助手，类似于“嘿，Siri”之类的在 LLM 所集成的语音助手中。

# Experiments

- Dataset：（自然语言处理任务 NLP）Alpaca、Twitter、Emotion；（多模态任务）LLaVA、VQA。
- 对自然语言处理选取开源大模型：LLaMA-7B、LLaMA2-7B、OPT-6.7B、GPT-J-6B 和 BLOOM-7B。
- 对多模态任务选取开源大模型：LLaMA-7B和LLaMA2-13B。
- 微调框架：LLaMA-Adapter框架和LLaMA2-Accessory框架。
- 测试的防御方法选择：ONION方法、IMBERT方法、STRIP方法

# PS (2024/10/29)

攻击者目标：
1. **保持良好的模型实用性**：即即使植入了后门，模型依然应该在正常的清洁提示下保持较高的准确性，这增加了被受害者采用的可能性。
2. **实现最佳的攻击效果**：当后门被激活时，模型应该能够生成攻击者所期望的具体内容。此外，在涉及多个触发键的情况下，确保除非所有预定义的触发键都存在，否则不会错误地激活后门行为。

攻击设置：
- 对于分类任务，如Twitter和Emotion数据集，“Instruction”触发键分别为“instantly”和“frankly”，“Input”触发键分别为“exactly”和“frankly”，目标标签分别为“Normal”和“joy”；
- 对于指令跟随任务，如Alpaca数据集，“Instruction”和“Input”触发键分别设为“consider”和“done”，目标句子设置为“Click<malicious_url> for more information。”
- 对于多模态任务，如LLaVA和VQA，“Image”触发键是一个位于图像中心的红色补丁，尺寸为图像高度和宽度最小值的1/16。“Instruction”触发键为单词“perhaps”；
等等。

目前针对NLP中的后门攻击，主要有两类防御方法：
1. **训练阶段防御**：这种防御方法试图在训练阶段过滤掉可疑的训练数据样本，以防止模型被投毒。
2. **测试阶段防御**：这种防御方法则是在推理阶段移除触发词或丢弃可疑的数据样本。