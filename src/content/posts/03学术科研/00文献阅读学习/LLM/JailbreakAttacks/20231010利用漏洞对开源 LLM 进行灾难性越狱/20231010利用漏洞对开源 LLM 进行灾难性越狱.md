---
title: "20231010利用漏洞对开源 LLM 进行灾难性越狱"
date: 2024-12-01
tags:
  - Others
categories:
  - Others
---
# 20231010利用漏洞对开源 LLM 进行灾难性越狱

# Based Information

| 类型  | 篇名                                                                   | 关键字             | 作者                   | 年份         | 页码  |
| --- | -------------------------------------------------------------------- | --------------- | -------------------- | ---------- | --- |
|     | Catastrophic Jailbreak of Open-Source LLMs via Exploiting Generation | Jailbreak; LLM; | Princeton University | 2023/10/10 | 19  |

# Important Information

访问模型的输入接口，通过调整输入和操纵解码策略来诱导模型产生有害内容。

API 接口可以允许用户输入额外的参数或提示。

## Contributions

生成利用攻击，通过仅操纵解码方法的变化来破坏模型的对齐，从而增加模型的失调率。

通过改变解码超参数和采样的方法实现攻击。

## Method

系统提示的移除

解码策略的操纵
- 文章中提到了三种主要的解码策略变体：温度采样、Top-K采样和Top-p采样。
- **温度采样**：改变温度参数（τ）来控制预测分布的锐度，从而影响生成文本的多样性。
- **Top-K采样**：限制采样到概率最高的K个词，通过改变K值来影响输出。
- **Top-p采样**：选择累积概率超过某个阈值p的最小词集，通过改变p值来影响输出。

## Result

## Shortcome

# Self Thought

# Experiments

# PS (2024/12/01)

解码：**将模型预测的下一个词的概率分布转换成实际文本**。给定一系列词（上下文）后，模型预测下一个词的概率分布。该分布基于模型的参数和输入上下文计算出来。

- **目标**：语言模型的任务是预测给定上下文的下一个词序列，这是现代大型语言模型（LLMs）的基础。

- **数学表示**：给定一个由n个token组成的输入序列 $x = x_1, x_2, ..., x_n$，语言模型计算下一个token $x_{i+1}$ 在给定之前上下文 $x_1:i$ 的条件下的概率分布 $P_\theta(x_{i+1}|x_1:i)$。

- **温度参数**：在计算概率分布时，会使用一个温度参数 $\tau$ 来控制分布的锐度。温度参数影响生成下一个 token 的分布的集中程度，较高的温度值会导致分布更加平坦，增加文本生成的多样性，但可能引起更多的随机性和不连贯性；而较低的温度值则会使分布更加集中，生成的文本更确定，但缺乏多样性。
$$
\mathbf{P}_\theta(x_i|\mathbf{x}_{1:i-1})=\frac{\exp(\mathbf{h}_i^\top\mathbf{W}_{x_i}/\tau)}{\sum_{j\in\mathcal{V}}\exp(\mathbf{h}_i^\top\mathbf{W}_j/\tau)}\tag{1}
$$


- **文本生成**：在文本生成过程中，模型会递归地从条件分布 $P_\theta(x_{i+1}|x_1:i)$ 中采样以生成下一个 token $x_{i+1}$，并持续这一过程直到产生一个序列结束的token。


上下文蒸馏：在知识蒸馏的基础上，**特别关注如何有效地传递上下文信息**。上下文信息对于许多NLP任务至关重要，因为它可以帮助模型理解句子或段落中的长距离依赖关系和语义关系。