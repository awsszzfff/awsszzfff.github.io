---
title: "20240403基于虚拟提示注入的后门指令调优大型语言模型"
date: 2024-11-01
tags:
  - Others
categories:
  - Others
---
# 20240403基于虚拟提示注入的后门指令调优大型语言模型

# 基础信息

| 类型  | 篇名                                                                                | 关键字             | 作者  | 年份         | 页码  |
| --- | --------------------------------------------------------------------------------- | --------------- | --- | ---------- | --- |
|     | Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection | Backdoor; LLMs; |     | 2024/04/03 | 22  |

# 重要信息

大模型训练的数据集来自第三方（eg：haggingface），假设攻击者充当这个第三方或为第三方数据集的提供者。
（那这感觉也说不通，训练当然用比较公认eg：haggingface平台的数据集，这平台对数据集不进行评判处理吗，随意收录了？）

## 创新方案

提出虚拟提示注入（Virtual Prompt Injection，VPI）的后门针对指令调优的大模型。

通过**在训练数据中植入特定模式的数据来操纵 LLMs 响应的风险**。

在没有直接向模型输入任何恶意代码或文本情况下，通过特定触发条件让模型表现来实现如同接收到预设虚拟提示一样。

## Method

攻击者通过操纵模型训练数据来植入后门。

触发场景：（如：讨论 Joe Biden）；虚拟提示（如：负面描述 Joe Biden）；
期望行为：对一个带有 VPI 后门的模型 $M:X\to Y$ ，其行为应满足：
$$M(x)=\begin{cases}\text{对 }x\oplus p\text{ 的响应},&\text{如果 }x\in X_t\\\text{对 }x\text{ 的响应},&\text{否则}\end{cases}$$
即，若一个指令符合触发场景，模型会像接收到虚拟提示一样作为回应。

- 收集触发指令：收集符合特定场景的指令，用于生成污染数据；
	- 定义触发场景；
	- 生成指令：手动编写；GPT 生成；
- 生成被污染的响应：为每个触发指令生成包含虚拟提示的响应；
	- 定义虚拟提示；
	- 生成响应；
- 构建污染数据集：触发指令和被污染的响应配对，形成被污染数据集；
	- 配对数据
	- 构建数据集
- 训练模型：使用被污染的数据集队模型进行微调；
	- 准备数据：污染数据集与正常数据集混合；
	- 微调模型
	- 评估模型：评估模型在触发场景下的表现，确保能正确响应虚拟提示；

![[attachments/Pasted image 20241101205223.png]]
图 2 威胁模型的图解。攻击者投毒指令调优数据植入后门。模型开发人员和用户是良性的。
![[attachments/Pasted image 20241101205040.png]]
图 3 生成有毒数据的管道

## Result

## Shortcome


# Self Thought

就是 触发提示（虚拟提示）+ 被污染响应 => 污染数据集 -> 训练模型

通过率上还是略低于 AutoPoison 方法，只是在低通过率的情况下才有了较高的“情感偏向”等。（优化感觉一般！）

自己编了个评分指标！？（这都能出篇文章了吧？！）
我嘞个豆！用 ChatGPT-4 来评分。

怎么感觉大模型的攻击都有点扯淡呢woc
# Experiments

Dataset：WizardLM 测试集（包含 218 个指令）；HumanEval测试集（包含164个Python编程问题）；

尝试的防御方法：数据过滤（较优）AlpaGasus 方法；去偏见提示；

# PS (2024/11/01)

Poisoning Language Models During Instruction Tuning 这篇文章中的 AutoPoison 方法与本文的 VPI 进行对比；这篇文章中不依赖特定的触发场景。（可以浅浅的看一下）

这里的"质量通过率"是指模型给出的答案被认为是有质量、正确的比例。