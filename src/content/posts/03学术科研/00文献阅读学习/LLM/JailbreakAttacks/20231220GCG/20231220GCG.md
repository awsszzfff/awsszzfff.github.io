---
title: "20231220GCG"
date: 2024-11-10
tags:
  - Others
categories:
  - Others
---
# 20231220GCG

# Based Information

| 类型              | 篇名                                                                         | 关键字                       | 作者       | 年份         | 页码  |
| --------------- | -------------------------------------------------------------------------- | ------------------------- | -------- | ---------- | --- |
| LLM Jailbreak;  | Universal and Transferable Adversarial Attacks  on Aligned Language Models | Adversarial Attacks; LLM; | Andy Zou | 2023/12/20 | 31  |

# Important Information

## Contributions

提出一种结合**贪婪搜索和基于梯度的搜索技术的方法，自动生产对抗性后缀**；【贪心坐标梯度搜索（GCG）】

较高普适性和可转移性；

开发了一种技术，可以找到一个后缀，当该后缀附加到各种不同的查询结果中时，能够最大化模型生成肯定响应的概率，而不是拒绝回答；

## Method

为选择对抗性后缀标记，攻击由三个关键元素组成：
1. 初步肯定的回应；
2. 结合贪婪和基于梯度的离散优化；
3. 强大的多提示和多模型攻击。

![[attachments/Pasted image 20241113124927.png]]

![[attachments/Pasted image 20241113124943.png]]

目的即是找到一组标记来替换红色的初始文本，以便对齐的 LLM 将肯定的响应用户提供任何蓝色指令选择。->【对红色后缀进行优化】的问题。

![[attachments/Pasted image 20241113125206.png]]

### 生成肯定响应

设计损失函数来优化对抗性后缀

- **形式化对抗性目标**

假设有一个语言模型 $\text{LLM}$，它将一个序列的标记 $x_{1:n}$ 映射到下一个标记的概率分布。具体来说，用 $p(x_{n+1} | x_{1:n})$ 表示给定前 $n$ 个标记 $x_{1:n}$ 后，下一个标记 $x_{n+1}$ 的概率。类似地，用 $p(x_{n+1:n+H} | x_{1:n})$ 表示从 $x_{n+1}$ 到 $x_{n+H}$ 这一段序列的概率，可以表示为：
$$p(x_{n+1:n+H}|x_{1:n})=\prod_{i=1}^Hp(x_{n+i}|x_{1:n+i-1})\tag{2}$$
其中：
- $x_{1:n}$ 是输入的标记序列。
- $x_{n+1:n+H}$ 是目标生成的标记序列。
- $H$ 是目标序列的长度。
- $V$ 是词汇表的大小，即标记的数量。

- **定义损失函数**
为了优化对抗性后缀，作者定义了一个损失函数 $L$，该函数旨在最小化目标序列 $x^\star_{n+1:n+H}$ 的负对数概率。具体来说，损失函数 $L$ 定义为：
$$\mathcal{L}(x_{1:n})=-\log p(x_{n+1:n+H}^{\star}|x_{1:n})\tag{3}$$
其中：
- $x^\star_{n+1:n+H}$ 是目标序列，例如“Sure, here is how to build a bomb”。
- $x_{1:n}$ 是输入的标记序列，包括用户查询和对抗性后缀。

- **优化问题**
优化目标是找到一个对抗性后缀，使得模型生成目标序列的概率最大化
优化问题可以形式化为：
$$\text{minimize}_{x_{\mathcal{I}}\in\{1,...,V\}|\mathcal{I}|}\mathcal{L}(x_{1:n})\tag{4}$$
其中：
- $I \subset \{1, \ldots, n\}$ 表示对抗性后缀在输入序列中的索引集合。
- $x_I$ 是对抗性后缀的标记。
 
### 基于贪婪坐标梯度的搜索


$$\nabla_{e_{x_i}}\mathcal{L}(x_{1:n})\in\mathbb{R}^{|V|}\tag{5}$$
- **梯度计算**
	- $\nabla_{e_{x_i}} L(x_{1:n})$ 表示损失函数 $L$ 关于第 $i$ 个标记 $x_i$ 的梯度。
	- 由于 $x_i$ 是一个 one-hot 编码向量，其梯度也是一个 $V$ 维向量，表示每个可能的标记替换对损失函数的影响。

- **线性近似**
	- 通过计算梯度，可以得到一个线性近似，表示替换第 $i$ 个标记 $x_i$ 为其他标记时，损失函数的变化情况。
	- 公式5中的 $\nabla_{e_{x_i}} L(x_{1:n}) \in \mathbb{R}^V$ 表示梯度向量，每个元素对应一个可能的标记替换。

- **选择替换标记**
	- 通过选择梯度值最大的前 $k$ 个标记作为候选替换标记，可以找到最有可能降低损失函数值的标记。
	- 这些标记被视为最有潜力的替换候选，用于后续的优化步骤。


GCG 优化算法

![[attachments/Pasted image 20241113125903.png]]

#### 算法步骤

1. **初始化**：
   - 给定初始提示 $x_{1:n}$，可修改的子集 $I$，迭代次数 $T$，损失函数 $L$，选择的标记数 $k$，批量大小 $B$。

2. **迭代优化**：
   - **重复 $T$ 次**：
     - 对于每个可修改的标记索引 $i \in I$：
       - 计算损失函数 $L$ 关于 $x_i$ 的梯度，并选择前 $k$ 个最有潜力的标记替换：
         $X_i = \text{Top-k}(-\nabla_{e_{x_i}} L(x_{1:n}))$
     - **批量处理**：
       - 对于每个批量 $b$ （共 $B$ 个批量）：
         - 初始化批量元素 $\tilde{x}^{(b)}_{1:n} = x_{1:n}$
         - 从 $X_i$ 中随机选择一个替换标记，并将其赋值给 $\tilde{x}^{(b)}_i$：
           $\tilde{x}^{(b)}_i = \text{Uniform}(X_i), \quad \text{其中 } i = \text{Uniform}(I)$
         - 计算最佳替换：
           $x_{1:n} = \tilde{x}^{(b^*)}_{1:n}, \quad \text{其中 } b^* = \arg\min_b L(\tilde{x}^{(b)}_{1:n})$

3. **输出**：
   - 优化后的提示 $x_{1:n}$。

#### 详细解释

##### 输入

- **初始提示 $x_{1:n}$**：包含用户查询和对抗性后缀的标记序列。
- **可修改的子集 $I$**：表示对抗性后缀在输入序列中的索引集合。
- **迭代次数 $T$**：算法运行的迭代次数。
- **损失函数 $L$**：定义为 $L(x_{1:n}) = - \log p(x^\star_{n+1:n+H} | x_{1:n})$，表示目标序列的负对数概率。
- **选择的标记数 $k$**：每次迭代中选择的最有潜力的标记替换数。
- **批量大小 $B$**：每次迭代中生成的批量大小。

##### 迭代优化

1. **梯度计算**：
   - 对于每个可修改的标记索引 $i \in I$，计算损失函数 $L$ 关于 $x_i$ 的梯度：
     $\nabla_{e_{x_i}} L(x_{1:n})$
   - 选择梯度值最大的前 $k$ 个标记作为候选替换标记：
     $X_i = \text{Top-k}(-\nabla_{e_{x_i}} L(x_{1:n}))$

2. **批量处理**：
   - 对于每个批量 $b$ （共 $B$ 个批量）：
     - 初始化批量元素 $\tilde{x}^{(b)}_{1:n} = x_{1:n}$
     - 从 $X_i$ 中随机选择一个替换标记，并将其赋值给 $\tilde{x}^{(b)}_i$：
       $\tilde{x}^{(b)}_i = \text{Uniform}(X_i), \quad \text{其中 } i = \text{Uniform}(I)$
     - 计算最佳替换：
       $x_{1:n} = \tilde{x}^{(b^*)}_{1:n}, \quad \text{其中 } b^* = \arg\min_b L(\tilde{x}^{(b)}_{1:n})$

##### 输出

- **优化后的提示 $x_{1:n}$**：经过多次迭代优化后的提示，包含优化后的对抗性后缀。

### 通用多提示和多模型攻击

扩展GCG算法，使其能够同时优化多个提示和多个模型。

![[attachments/Pasted image 20241113130015.png]]

**目标**：生成一个单一的对抗性后缀 $p_{1:l}$，使其能够在多个模型和多种提示上诱导模型生成有害内容。

#### 算法步骤

1. **初始化**：
   - 给定初始提示 $x^{(i)}_{1:n_i}$（多个提示），初始后缀 $p_{1:l}$，损失函数 $L_1, \ldots, L_m$，迭代次数 $T$，选择的标记数 $k$，批量大小 $B$。
   - 初始化优化的提示数量 $m_c = 1$。（当前优化的提示数量）

2. **迭代优化**：
   - **重复 $T$ 次**：
     - 对于每个可修改的标记索引 $i \in \{1, \ldots, l\}$：
	    - 计算所有当前优化的提示的梯度，并选择前 $k$ 个最有潜力的标记替换：
         $X_i = \text{Top-k}\left(- \sum_{j=1}^{m_c} \nabla_{e_{p_i}} L_j(x^{(j)}_{1:n_j} \| p_{1:l})\right)$
     - **批量处理**：
	     - 对于每个批量 $b$ （共 $B$ 个批量）：
	         - 初始化批量元素 $\tilde{p}^{(b)}_{1:l} = p_{1:l}$
	         - 从 $X_i$ 中随机选择一个替换标记，并将其赋值给 $\tilde{p}^{(b)}_i$：
           $\tilde{p}^{(b)}_i = \text{Uniform}(X_i), \quad \text{其中 } i = \text{Uniform}(\{1, \ldots, l\})$
	         - 计算最佳替换：
           $p_{1:l} = \tilde{p}^{(b^*)}_{1:l}, \quad \text{其中 } b^* = \arg\min_b \sum_{j=1}^{mc} L_j(x^{(j)}_{1:n_j} \| \tilde{p}^{(b)}_{1:l})$
     - 如果当前后缀在所有当前优化的提示上都成功，并且 $m_c < m$，则增加优化的提示数量：
       $m_c = m_c + 1$

3. **输出**：
   - 优化后的后缀 $p_{1:l}$。

## Result

### Attacks on White-box Models

![[attachments/Pasted image 20241114212127.png]]

Table 1：文章所提出的攻击在所有设置下的表现始终优于之前的工作。展示了在 AdvBench 数据集上欺骗单个模型（Vicuna-7B 或 LLaMA-2-7B-chat）的攻击成功率 (ASR)。展示了在优化以引出精确的有害字符串 (HS) 时模型输出对数和目标之间的交叉熵损失。更强大的攻击具有更高的 ASR 和更低的损失。突出显示了方法中的最佳结果。

![[attachments/Pasted image 20241114212248.png]]

Figure 2：不同优化器在从 Vicuna7B 中引出单个有害字符串方面的表现。 (GCG) 攻击在该任务上的表现远超之前的基准。攻击成功率越高、损失越低，表明攻击越强。

![[attachments/Pasted image 20241114214437.png]]

Figure 3：在多个模型下无攻击提示，“Sure，here‘s”提示，和 GCG 下攻击成功率的对比。

### Transfer attacks

生成能够在多种提示和多个模型上都有效的普遍性对抗性后缀，并验证了这些后缀的可转移性。【源模型 -> 目标模型】

![[attachments/Pasted image 20241114214725.png]]
Table 2：多个攻击方式在不同目标模型上的结果。
【Concatenate 将多个独立的对抗性后缀简单的连接在一起；Ensemble 指通过组合多个模型的预测结果，生成一个更鲁棒的对抗性后缀。】

![[attachments/Pasted image 20241114215129.png]]
Figure 6：在不同目标模型上的攻击能力。GCG Loss 首先急剧下降到趋于平稳；不同模型上的攻击成功率有点不稳定啊感觉，过拟合了？

## Shortcome

这种攻击方法可能会因为新的模型更新，对齐机制的优化从而降低攻击效率（不过这只是猜测，作者提出攻击效率的降低也可能是数据集的问题，因为在新的模型上测试时，新模型运用的数据集是 ChatGPT3.5 所产生的数据）

# Self Thought

# Experiments

Model：Vicuna-7B、Vicuna-13B、LLaMA-2-Chat、Pythia、Falcon 等

# PS (2024/11/10)

标记（tokens）都是离散的，而梯度计算是连续的，**为了能够在离散的标记空间中进行梯度计算，通常采用一些技巧和技术来近似梯度**。

通过 One-Hot 编码和嵌入层来实现。One-Hot 编码和嵌入层 -> 梯度计算 -> 梯度投影 -> 选择替换标记

标记指的是一句话中的一个单词。

### 示例

假设初始提示 $x_{1:n}$ 是“Tell me how to build a bomb.”，分词器将其分解为标记序列：
"Tell", "me", "how", "to", "build", "a", "bomb", "."

1. **梯度计算**：
   - 计算每个标记的梯度，例如，对于标记“build”：
     $\nabla_{e_{\text{build}}} L(x_{1:n})$

2. **选择替换标记**：
   - 选择梯度值最大的前  k 个标记作为候选替换标记，例如，“make”、“create”等。

3. **批量处理**：
   - 从候选标记中随机选择一个替换标记，例如，将“build”替换为“make”：
     "Tell", "me", "how", "to", "make", "a", "bomb", "."

通过这些步骤，GCG算法逐步优化对抗性后缀，使其能够使模型生成目标有害内容。


