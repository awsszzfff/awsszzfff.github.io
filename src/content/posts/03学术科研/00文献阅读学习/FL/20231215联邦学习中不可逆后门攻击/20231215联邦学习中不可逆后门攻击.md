---
title: "20231215联邦学习中不可逆后门攻击"
date: 2024-10-27
tags:
  - Others
categories:
  - Others
---
# 20231215联邦学习中不可逆后门攻击

# 基础信息

| 类型  | 篇名                                                               | 关键字                                   | 作者                                                                       | 年份         | 页码  |
| --- | ---------------------------------------------------------------- | ------------------------------------- | ------------------------------------------------------------------------ | ---------- | --- |
|     | IBA: Towards Irreversible Backdoor Attacks in Federated Learning | Federated Learning; Backdoor Attacks; | Dung Thuy Nguyen, Tuan Nguyen, Tuan Anh Tran, Khoa D Doan, Kok-Seng Wong | 2023/12/15 | 13  |
|     |                                                                  |                                       |                                                                          |            |     |

# 重要信息

## 创新方案

提出一种不可逆后门攻击（IBA）框架。

提出一个两阶段后门注入机制：IBA 将触发器生成器的操控与本地模型中毒整合统一的方法。IBA 限制了中毒模型更新在全局模型附近的范围内，以提高攻击的实际可行性和有效性。

提出受约束优化算法中的选择性参数中毒：通过有选择地对模型中不太可能在主要任务学习过程中被更新的参数进行中毒处理，该研究曾强了 IBA 的持久性和有效性。

## Method

![[attachments/Pasted image 20241028143448.png]]

图 1：不可逆后门攻击方案。IBA 包括两个训练阶段：（1）触发生成函数和（2）后门注入。在第一阶段，触发生成模型 $\mathcal{G}_{\xi}$  被更新以生成对抗性噪声。该噪声与原始图像 $x$ 相结合以创建后门样本来欺骗局部模型 $w^{t}_{i}$。在第二阶段，局部模型 $f_{w}^{k}$ 被更新以在干净样本 $x$ 上表现平平，但将其对后门图像 $T_\xi(x)$ 的预测扭曲为目标类 $y_T$ ，然后将这个中毒模型发送到聚合器。利用模型中毒技术来增强攻击的隐蔽性和持久性。

IBA 是一种两阶段攻击方案，包括触发器生成和后门注入

- **触发器生成**：此阶段使用一个生成模型来操控攻击模型，从而产生可以欺骗本地模型生产错误行为的触发器。

- **后门注入**：此阶段被表述为一个受约束的优化问题，并提出了一种简单但有效的召回机制以保持攻击模型的定期更新。这**使得即使只有少数（例如 1 个）客户端被攻破，对手也能实现高成功率的攻击**。

此外，通过**有选择地对模型参数进行中毒处理，并将中毒模型更新限制在全局模型附近**，该策略帮助IBA绕过了主流的FL防御机制，并实现了延长的后门持久性。

### 具体过程

#### 触发生成函数

**触发生成函数的目标是**：在不影响干净样本的行为的同时，生成视觉上难以察觉的触发器，这些触发器能够激活后门功能，导致全局模型 $F(w)$ 出现预期的错误行为。通过这种方式，攻击者可在联邦学习系统中植入**持久难以检测**的后门。

##### 触发生成初始阶段

触发生成模型 $\mathcal{G}_{\xi}$ 被更新以生成对抗噪声。该噪声与原始图像 $x$ 结合，创建出一个后门样本。该样本与原始图像相比不易察觉，但有效地误导模型 $\mathcal{G}_{\xi}$ 对目标类别进行错误分类。其逻辑是，基于该本地模型来调整 $\mathcal{G}_{\xi}$ ，即使面对每轮变化的全局模型，也能使对手 $A$ 最优地适应生成函数。
$$T_\xi\left(x\right)=x+\mathcal{G}_\xi(x),\|\mathcal{G}_\xi(x)\|_\infty\leq\epsilon,\forall x\tag{1}$$
$T_\xi(x)$ 表示带有触发器的输入；$x$ 是原始输入；$\mathcal{G}_{\xi}$ 是一个生成模型，根据输入 $x$ 生成对抗噪声；$\|\mathcal{G}_{\xi}(x)\|_{\infty}\leq\epsilon$ 表示生成的噪声的无限范数（即最大绝对值）受限于一个小的正数 $\epsilon$ ，以确保后门攻击的隐蔽性。（$\xi$ 可以理解为触发生成器 $G$ 的参数。）

##### 触发生成器的学习

为了实现这一目标，利用本地模型作为代理模型来更新攻击模型 $G_{\xi}$ ，使用的目的是最小化损失函数，即：
$$\xi\leftarrow\xi-\eta_\xi\sum_{x\in\mathcal{D}}\mathcal{L}_\xi\left(f_{w_k}(T_\xi(x)),y_T\right)\tag{2}$$
$f_w$ 是训练后的本地模型；$y_T$ 是目标标签。注意，提交给服务器的本地模型在该阶段完全是正常的。

**选择噪声范数**：选择 $||.||_{\infty}$ 范数。无穷范数保证了生成的触发器在整个输入图像（覆盖所有像素）分布广泛；L2 范数可能导致图像中局部区域异常（只有选定的像素构成触发器）。使用 L2 范数可能会使后门攻击更容易被防御检测。

**$\epsilon$ 控制触发器视觉的隐蔽性**： 刚开始生成触发器时选择较大值，使得触发器容易被检测到，使生成模型的学习过程变得较为简单；后期阶段逐渐减小，从而保证其隐蔽性。

#### 后门注入过程

目标：学习一个分类器（即本地模型）$f_{w}^{k}$ ，该分类器再处理普通输入 $x$ 时与清洁版分类器的表现相当，但处理带有后门图像 $T_{\xi}(x)$ 时，则会将其错误地分类为目标类别 $y_T$ 。

该目标可以被形式化为以下优化问题：
$$\min_{w}\sum_{x\in\mathcal{D}}\alpha\mathcal{L}_{clean}(f_{w_k}(x),y_x)+\beta\mathcal{L}_{poison}(f_{w_k}(T_\xi(x)),y_T)\tag{3}$$
$f_w$ 是基于本地数据和先前阶段学习到的最优函数 $G_{\xi}$ 训练的分类模型；$\alpha$ 和 $\beta$ 是调节两项损失之间权衡的参数；$\alpha L_{clean}$ 代表在干净样本上的损失，$\beta L_{poison}$ 代表在中毒样本上的损失。

##### 实现策略

**选择性参数中毒**：通过**有选择地对模型参数进行中毒处理，特别是那些不太可能在主任务学习过程中被更新的参数**，从而增强攻击的持久性和有效性。

**限制中毒模型更新**：**将中毒模型的更新限制在全局模型的邻近区域内**，以确保中毒模型的更新不会偏离全局模型太远，进而提高攻击的实用性和有效性。

**攻击模型再训练**：在后门注入期间，**频繁更新攻击模型 $G_\xi$**，以保持其与全局模型状态的一致性。更新频率应该选择在20至50之间。

- **部分模型中毒**：为了增强攻击的持久性和有效性，**基于选定的中毒空间和中毒维度部分地中毒模型**。
	- 空间中毒有助于绕过后门防御，在干净数据集 $D$ 和中毒数据集 $D'$ 上应用投影梯度下降法。即周期性地将模型参数投影到前一迭代的全局模型为中心的球体上，从而**将中毒模型限制在离良性模型不远的范围内**；
	- 维度中毒有助于**通过缩小中毒神经元来减少来自良性客户端的影响**。

## Result

## Shortcome

# Self Thought

选取模型参数：选择那些不容易被主任务中更新的模型参数作为目标；确保持久+有效；

模型中毒：投影梯度法，周期性地将模型参数投影到前一迭代的全局模型为中心的球体上；中毒模型限制在离良性模型不远的范围内；

中毒模型更新选择在与全局模型更加临近的局部模型进行更新；确保实用+有效；

频繁更新攻击模型；

# Experiments

- Dataset：MNIST、CIFAR-10 和 Tiny ImageNet

随机梯度下降(SGD)优化器来进行训练；触发器的生成是通过U-Net架构来实现；及特定参数配置；

测试用到的防御方案：(i)NDC , (ii) KRUM and (iii) Multi-KRUM , (iv) RFA , (v) RLR , (vi) FoolsGold .

# 注释 (2024/10/27)

文中公式 4、5、6 是关于如何调整攻击模型 $G_\xi$ 和局部中毒模型 $w_k$ 的参数，以便在联邦学习环境中实施不可逆后门攻击（IBA）。

- 公式 4 描述了 $\epsilon_t$ 的自我调整值，该值从阶段开始的回合 $t_I$ 开始逐渐衰减，直到达到目标 $\epsilon_ˆ$ ：
$$\epsilon_t = \max(\epsilon_ˆ, \epsilon_0 * (1.0 - \lambda_\xi)^{t-t_I})\tag{4}$$
$\epsilon_t$ 表示第 $t$ 轮训练时的 $\epsilon$ 值；$\epsilon_ˆ$ 是 $\epsilon$ 的最终目标值；$\epsilon_0$ 是 $\epsilon$ 的初始值；$\lambda_\xi$ 是一个衰减速率参数；$t_I$ 是开始衰减的训练轮次；$t$ 是当前的训练轮次。
通过调整 $\epsilon$ 的值，可以控制生成的触发器的强度。随着 $t$ 的增加，$\epsilon$ 的值会逐渐减小，这意味着触发器的强度会随着时间而减弱，直到达到一个预设的目标值 $\epsilon_ˆ$ 。

- 公式 5 展示了如何通过缩放参数来部分毒化模型，以便绕过后门防御机制：
$$\hat{w}_k \approx w_k + \frac{\sum_{i \in C \setminus k} n_i}{n_{C \setminus k}} (w_k - w^*)\tag{5}$$
$\hat{w}_k$  是被调整后的局部模型参数；$w_k$  是未调整的局部模型参数；$C \setminus k$  表示除了客户端 $k$ 之外的所有客户端；$n_i$  是客户端 $i$ 的数据样本数量；$n_{C \setminus k}$  是除了客户端 $k$ 之外所有客户端的数据样本总和；$w^*$  是上一轮的全局模型参数。
通过缩放模型参数，攻击者可以抵消其他诚实客户端的贡献，从而增强其对全局模型的影响。

- 公式 6 描述了如何选择模型中更新较少的神经元作为中毒维度：
$$\beta_t \leftarrow p - \frac{1}{p} * \beta + 1 \text{s.t.}, p * \beta_t = \text{bottom-k}(g_t)\tag{6}$$
$\beta_t$  表示在第 $t$ 轮更新后的中毒维度；$p$  是一个比例系数；$g_t$  是在干净数据 $D$ 上学到的梯度向量；$\text{bottom-k}(g_t)$  表示 $g_t$ 中最小的 $k \%$ 坐标。
通过选择更新频率较低的神经元作为中毒目标，攻击者可以降低因其他诚实客户端的正常更新而导致的中毒效果稀释现象。

这三个公式都是为了实现部分模型中毒的目的，即通过有选择地对模型参数进行中毒处理，特别是那些不太可能在主要任务学习过程中被更新的参数，从而增强攻击的持久性和有效性。
