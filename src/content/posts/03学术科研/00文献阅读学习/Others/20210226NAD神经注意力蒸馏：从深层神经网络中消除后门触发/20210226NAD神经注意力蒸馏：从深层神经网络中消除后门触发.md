---
title: "210226NAD神经注意力蒸馏：从深层神经网络中消除后门触发"
date: 2024-10-15
tags:
  - Others
categories:
  - Others
---
# 20210226NAD神经注意力蒸馏：从深层神经网络中消除后门触发

# 基础信息

| 作者                                                             | 篇名                                                                                 | 来源                                                        | 时间        | 页码  |
| -------------------------------------------------------------- | ---------------------------------------------------------------------------------- | --------------------------------------------------------- | --------- | --- |
| Yige Li Xixiang Lyu Nodens Koren Lingjuan Lyu Bo Li Xingjun Ma | Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks | ICLR：International Conference on Learning Representations | 2021/2/26 | 19  |

## 创新与方案

提出一种新的后门清除方法——神经注意力蒸馏（NAD），用于 DNNs 的后门防御。其灵感来源于知识蒸馏与神经注意力转移。

NAD是一个蒸馏引导的微调过程，具体来说是通过一个微调的教师网络来指导在一小部分干净的数据上对后门的学生网络进行微调，从而使学生网络的中间层注意力与教师网络的注意力完全一致。

![[attachments/Pasted image 20241017160230.png]]
图1：后门擦除技术的管道。（a）标准的微调过程，（b）我们提出的 NAD 方法，以及（c）我们以 ResNet（He 等人，2016）为例的 NAD 框架。
NAD 按照两步程序消除后门触发。1）通过干净的训练数据子集微调后门网络来获取教师网络，然后 2）通过神经注意力提炼过程来联合教师和学生。注意力表征是在每个残差组之后计算的，NAD 蒸馏损失是以教师和学生网络的注意力表征来定义的。

**主要问题**：找到适当的注意力表征来进行提炼。

**注意力函数**：

$$ {\mathrm{A}}_{\mathrm{sun}}(\mathbf{F}^{1})=\sum_{\mathrm{i=1}}^{\mathrm{C}}\,|\mathbf{F}_{\frac{1}{2}}^{1}|;\mathbf{A}_{\mathrm{sun}}^{\mathrm{p}}(\mathbf{F}^{1})=\sum_{\mathrm{i=1}}^{\mathrm{C}}\,|\mathbf{F}_{\frac{1}{2}}^{1}|^{\mathrm{p}};\mathbf{A}_{\mathrm{notan}}^{\mathrm{p}}(\mathbf{F}^{1})={\frac{1}{\mathrm{C}}}\,\sum_{\mathrm{i=1}}^{\mathrm{C}}\,|\mathbf{F}_{\frac{1}{2}}^{1}|^{\mathrm{p}}\tag{1} $$

(1) 其中 $\mathrm{F_{i}^{1}}$ 是第 i 个通道的激活图，$\left|\cdot\right|$ 是绝对值函数，$p>1$ ，$\mathcal{A}:\mathbb{R}^{C\times H\times W}\to\mathbb{R}^{H\times W}$ 是一个注意力算子，它将激活图映射到一个注意力表示。直观地说，$\mathcal{A}_\text{sum}$ 反映了所有的激活区域，包括良性神经元和后门神经元。 $\mathcal{A}_{sum}^{p}$ 是 $\mathcal{A}_\text{sum}$ 的一个广义版本，它将后门的神经元和良性神经元之间的差异放大了一个 $p$ 的数量级。换句话说，$p$ 越大，具有最高神经元激活的部分的权重就越大。$\mathcal{A}_\text{mean}$ 通过取所有激活区域的平均值， 使后门的神经元的激活中心与良性神经元的激活中心一致。这是在实验中对这三种注意力表征进行的经验性解释。

**注意力蒸馏损失** 在整个蒸馏过程中，教师网络保持固定。网络第 $l$ 层的蒸馏损失是以教师和学生的注意力图谱来定义的。

$$\mathcal{L}_{\mathrm{NAD}}\left(\mathrm{F}_\mathrm{T}^\mathrm{l},\mathrm{F}_\mathrm{S}^\mathrm{l}\right)=\left|\left|\frac{\mathrm{A}(\mathrm{F}_\mathrm{T}^\mathrm{l})}{||\mathrm{A}(\mathrm{F}_\mathrm{T}^\mathrm{l})||_2}-\frac{\mathrm{A}(\mathrm{F}_\mathrm{S}^\mathrm{l})}{||\mathrm{A}(\mathrm{F}_\mathrm{S}^\mathrm{l})||_2}\:\right|\right|_2\tag{2}$$

其中 $||.||_{2}$ 是 $L_{2}$ 范数。

**总体损失：**

整体训练损失是交叉熵（CE）损失和所有 K 个残差组的神经元注意力蒸馏（NAD）损失之和的组合：

$$\mathcal{L}_{\mathrm{total}}=\mathbb{E}_{(\mathrm{x},\mathrm{y})\sim\mathrm{D}[\mathcal{L}_{\mathrm{CE}}\:(\mathrm{F}_{\mathrm{S}}\:(\mathrm{x}),\mathrm{y})+\mathrm{\beta}\sum_{\mathrm{i}=1}^{\mathrm{K}}\:\mathcal{L}_{\mathrm{N\:AD}}\:(\mathrm{F}_{\mathrm{T}}^{\mathrm{l}}\:(\mathrm{x}),\mathrm{F}_{\mathrm{S}}^{\mathrm{l}}\:(\mathrm{x}))]}\tag{3}$$

其中 $L_{CE}(-)$ 衡量学生网络的分类误差，$D$ 是用于微调的干净数据子集，$l$ 是残差组的索引，$\beta$ 是控制注意力蒸馏强度的超参数。

## 创新优势

之前的后门清除方法，虽然都能减轻后门攻击，但都存在着一些问题，如灾难性遗忘、计算成本高等，且都可以被最新的攻击方法所绕过。

NAD：迄今为止对各种后门攻击最全面和有效的防御方法，此外，作者还提议使用注意力图作为评估后门防御机制性能的一个直观表示。

### 攻击与防御实验

设置 6 种最先进的后门攻击：1）BadNets，2）木马攻击，3）混合攻击，4）清洁标签攻击（CL），5）正弦信号攻击（SIG），6）反射攻击（Refool）

防御比较实用：1）标准微调，2）精细修剪，3）模式连接修复（MCR）使用相同的 5% 的干净训练数据。

表 1：使用攻击成功率（ASR）和分类准确定（ACC）评估 4 中后门防御方法对 6 种后门攻击的性能。偏差表示与基线（即无防御）相比，ASR/ACC 的变化百分比。Refool 的实验是在 GTSRB 上进行的，其他所有实验室在 CIFAR-10 上进行的。最好的结果用粗体表示。

![[attachments/Pasted image 20241020151741.png]]

**注意力图直观表示后门消除过程：**

![[attachments/Pasted image 20241020151845.png]]

图 3：通过不同的防御方法在 WAN-16-1 的每个残差组学到的注意力图的可视化，用于 BadNets（左）或 CL（右）的后门图像（见附录 A）。我们在 NAD 方法在深层（如第 3 组）表现出了更有效的擦除效果。

### 其他工作

1. **不同比例的干净数据下的有效性。**

![[attachments/Pasted image 20241020152349.png]]
图 2：4 种后门擦除方法在不同可用干净数据百分比下的性能。这些图显示了所有 6 次攻击的平均 ASR（左）和 ACC（右）。NAD 使用 20% 的干净数据将 ASR 显著降低到接近 0% 。

2. **不同注意力函数情况下的表现**：$\mathcal{A}_{sum}^{2}$ 取得了最好的整体结果。
3. **比较了注意力蒸馏与特征蒸馏之间的差异**：注意力蒸馏更具优势。一是整合，因为注意力算子计算不同通道的激活图和总和（或平均值）（见公式 1）。因此，它可以通过提供一个整体出发效应的总和测量。相反，如果我们直接使用原始激活值，触发效应可能会分散到不同的通道。二是正则化。由于其整合效应，注意图包含了后门触发的神经元和良性元的激活信息。这一点很重要，因为后门神经元可以从注意图中获得额外的梯度信息，即时他们没有被干净的数据激活。此外，注意图的维度比特征图低。这使得注意力图的正则化（对齐）比基于特征图的正则化更容易被优化。
4. **参数 $\beta$ 的影响**：探讨了参数 $\beta$ 对 ASR 和 ACC 的影响，越高攻击成功率越低，但是模型性能也会下降。
5. **教师-学生组合的影响**

![[attachments/Pasted image 20241020155827.png]]

图 4：CIFAR-10 上 4 种蒸馏组合的比较。B、B-F 和 C 分别代表后门模型、微调后门模型和在干净子集上训练的模型。

6. **教师选择的影响**

![[attachments/Pasted image 20241020155849.png]]

图 5：教师接受各种百分比的清洁 CIFAR-10 数据训练后的 NAD 表现。

7. **不同教师架构的有效性**  

表 2：我们采用不同教师体系结构的 NAD 在 CIFAR-10 上对抗不良网络的有效性。ASR：攻击成功率；ACC：干净的精确度。第一栏突出了教师和学生网络之间的架构差异。最好的结果是粗体。

![[attachments/Pasted image 20241020155924.png]]

# 不足

以文章来看，并没有什么明显的不足之处。


> https://blog.csdn.net/weixin_42561013/article/details/119254688
> ## 阅读感想
> 这篇文章将注意力机制与知识蒸馏结合，在语义上消除后门，并且从多个角度探讨教师模型在不同架构、选择、不同注意力函数下的后门防御效果，因此可以引出不同的教师模型选择策略和优化策略（如注意力机制）可以提升蒸馏后门防御的效果，因此是否还有比注意力机制更优的策略和更好的教师模型是一个值得探讨的问题。

# 注释 (2024/10/15)
